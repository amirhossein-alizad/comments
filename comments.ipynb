{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI CA3\n",
    "## Naive Bayes Classifiers\n",
    "## Amirhossein Alizad 810197546\n",
    "<strong> we are going to analyze the comments on a given shop as a train file and then predict for each comment in a given test file whether the user recommended it or not</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> by reading the comment train file, we calculate the probability of each word and divide them into two categories, recommended and not_recommended. then by doing the same for comments in test file and using bayes rule to find out whether it is recommended or not.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> using hazm library to normalize comments and pandas to read and operate on csv files</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#globals\n",
    "global sym\n",
    "sym = ['.', '!', '،', '?']\n",
    "\n",
    "global stopwrd\n",
    "stopwrd = stopwords_list()\n",
    "\n",
    "global RECOMMENDED\n",
    "RECOMMENDED = 0\n",
    "\n",
    "global NOT_RECOMMENDED\n",
    "NOT_RECOMMENDED = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> 'sym' is an array consisting of symbols that does not concern the recommendation of a product according to a comment and so we will ignore it.<br>\n",
    "just likethe sym, we have a list of stop words which are the words that happen alot and usually does not change the meaning of the sentence so we will ignore them too.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_comment(sentence):\n",
    "    normalizer = Normalizer()\n",
    "    sentence = normalizer.normalize(sentence)\n",
    "    lst1 =  word_tokenize(sentence)\n",
    "    stemmer = Stemmer()\n",
    "    lst2 = [stemmer.stem(i) for i in lst1]\n",
    "    return lst2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)\n",
    "<strong>to normalize a comment we can do 3 operations on a sentence or a word:<br>\n",
    "1)normalize: this function is used for replacing space with half-space. this function makes data proccesing easier<br><br>\n",
    "2)stemming: it creates the base of a word, may be wrong some times, but still helps to normalize the sentence. for example words: زیبایی and زیبا are two different words but when stemmed, they both turn into زیبا meaning that the sentence they were both in may have the same meaning.<br><br>\n",
    "3)lemmatization: just like stemming but works on verbs and returns the base form of a verb. for example میخواهم and میخواستم refer to a different time and technically are different but by lemmatizing they both turn into خواه#خواست causing them to have the same effect on a sentence.<br><br>\n",
    "stemming usually removes the extra end of a word like plural signs, etc.lemmatization also does the same and also returns the root and base of the word or a verb.one of the errors of stem is for exaple the words ending with م. the stemmer changes the word بادام into بادا causing it to be completely meaningless. and also lemmatizer takes much more time than stemming (this is concluded from runnig and testing with both functions and there were times that lemmatizer did not even finish the operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dictionary_without_smoothing(dic, lst, recommended):\n",
    "    for i in lst:\n",
    "        if not(i in stopwrd) and not(i in sym):\n",
    "            if i in dic[recommended]:\n",
    "                dic[recommended][i] += 1\n",
    "            else:\n",
    "                dic[recommended][i] = 1\n",
    "                \n",
    "def eval_dictionary_with_smoothing(dictionary, lst):\n",
    "    for i in lst:\n",
    "        if not(i in stopwrd) and not(i in sym):\n",
    "            if i not in dictionary[RECOMMENDED]:\n",
    "                dictionary[RECOMMENDED][i] = 1\n",
    "            else:\n",
    "                dictionary[RECOMMENDED][i] += 1\n",
    "    for i in lst:\n",
    "        if not(i in stopwrd) and not(i in sym):\n",
    "            if i not in dictionary[NOT_RECOMMENDED]:\n",
    "                dictionary[NOT_RECOMMENDED][i] = 1\n",
    "            else:\n",
    "                dictionary[NOT_RECOMMENDED][i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> given the normalized sentence, we evaluate our dictionary (wether we use smoothing or not) by the words in the sentence. if the word is not in stop words list and not a symbol then we can use it in our dictionary</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_p_without_smoothing(p, lst, rec_p, not_rec_p):\n",
    "    rec = p[RECOMMENDED]\n",
    "    not_rec = p[NOT_RECOMMENDED]\n",
    "    for i in lst:\n",
    "        if not(i in stopwrd) and not(i in sym):\n",
    "            if i in rec_p:\n",
    "                rec *= rec_p[i]\n",
    "            else:\n",
    "                rec = 0\n",
    "                break\n",
    "    for i in lst:\n",
    "        if not(i in stopwrd) and not(i in sym):\n",
    "            if i in not_rec_p:\n",
    "                not_rec *= not_rec_p[i]\n",
    "            else:\n",
    "                not_rec = 0\n",
    "                break\n",
    "    return rec, not_rec\n",
    "\n",
    "def calc_p_with_smoothing(lst, p, dictionary):\n",
    "    rec = p[RECOMMENDED]\n",
    "    not_rec = p[NOT_RECOMMENDED]\n",
    "    rec_p = dict()\n",
    "    not_rec_p = dict()\n",
    "    rec_count = [0]\n",
    "    not_rec_count = [0]\n",
    "    for i in dictionary[RECOMMENDED]:\n",
    "        rec_count[0] += dictionary[RECOMMENDED][i]\n",
    "    for i in dictionary[NOT_RECOMMENDED]:\n",
    "        not_rec_count[0] += dictionary[NOT_RECOMMENDED][i]\n",
    "    for i in dictionary[RECOMMENDED]:\n",
    "        rec_p[i] = dictionary[RECOMMENDED][i] / rec_count[0]\n",
    "    for i in dictionary[NOT_RECOMMENDED]:\n",
    "        not_rec_p[i] = dictionary[NOT_RECOMMENDED][i] / not_rec_count[0]\n",
    "    for i in lst:\n",
    "        if not(i in stopwrd) and not(i in sym):\n",
    "            rec *= rec_p[i]\n",
    "    for i in lst:\n",
    "        if not(i in stopwrd) and not(i in sym):\n",
    "            not_rec *= not_rec_p[i]\n",
    "    return rec, not_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> we calculate the probability of every word, if we're using smoothing,we find all the words that are not in the recommended and not recommended dictionary and increase them by 1 every time we see them. then the probability for every word is : $count_i / \\sum count_j$.<br>\n",
    "after calculating the probability of every word from train data, we can find wether a comment in test data is recommended or not.\n",
    "in smoothing we first find the words that are not in our dictionary and intialize them to 1, and every time we see it again, we increase it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)\n",
    "<strong>\n",
    "1) ‫‪evidence: P(comment) is the evidence which is not useful in this assignment.<br>\n",
    "2) likelihood: P(comment|recommended) is the likelihood. we calculate it by multiplying all P(word|recommended).<br>\n",
    "3) prior: P(recommended or not recommended) is the prior probability of class. we calculate it by counting all of the recommended or not recommended comments in test data and divide it by number of all comments or p(recommended) = 1 - p(not recommended)<br>\n",
    "4) posterior: P(recommended|comment) is the posterior probability. it is the probability of being recommended or not recommended given the comment. we calculate it by multiplying likelihood and prior.<br>\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)\n",
    "<strong>in naive bayes we have a bag of words which are fixed and do not change, when we find a word in a comment that is not for example in th recommended list, we say for sure that this comment is not recommended because <br>$p[recommended] = p[word] * p[recommended]$ and because we said that the word is nott in our list then $p[word] = 0$ thus $p[recommended] = 0$ so it is not_recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)\n",
    "<strong> additive smoothing is a technique to smooth data.it is used to solve the problem of zero probability. to implement it we add 1 to the count of all words in the train data, when a word is not in the train list, its probability will not be zero as its count is now 1 and when we see it again we will increase it so it is not zero any more and the problem is solved.it helps the problem in a way that when for example we have a comment: لذت بخش بود\n",
    "<br> but we dont have the word لذت in our list so the probability of recommended will be 0 and it will be resulted as not recommended. as you can understand this comment in a way says that it recommends the product but we predicted it as not recommended because of 0 probability. by using smoothing this problem is solved and the comment will be (with a higher probability) resulted as recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_pre(row, dic, rec_count, not_rec_count):\n",
    "    lst = normalize_comment(row['comment'])\n",
    "    comment_class = row['recommend']\n",
    "    if comment_class == \"recommended\":\n",
    "        recommended = RECOMMENDED\n",
    "        rec_count[0] += 1\n",
    "    else:\n",
    "        recommended = NOT_RECOMMENDED\n",
    "        not_rec_count[0] += 1\n",
    "    eval_dictionary_without_smoothing(dic, lst, recommended)\n",
    "    \n",
    "def train_without_pre(row, dic, rec_count, not_rec_count):\n",
    "    sentence = row['comment']\n",
    "    comment_class = row['recommend']\n",
    "    if comment_class == \"recommended\":\n",
    "        recommended = RECOMMENDED\n",
    "        rec_count[0] += 1\n",
    "    else:\n",
    "        recommended = NOT_RECOMMENDED\n",
    "        not_rec_count[0] += 1\n",
    "    lst =  word_tokenize(sentence)\n",
    "    eval_dictionary_without_smoothing(dic, lst, recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> these are the functions that evaluate dictionaries and process on the comments which as you can see the first on has a pre process function(normalize_comment()) and the second one does not</strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_pre_without_smooth(row, p, rec_p, not_rec_p, all_of, correct):\n",
    "    lst = normalize_comment(row['comment'])\n",
    "    comment_class = row['recommend']\n",
    "    rec, not_rec = calc_p_without_smoothing(p, lst, rec_p, not_rec_p)\n",
    "    if(rec >= not_rec):\n",
    "        all_of[0] += 1\n",
    "    if (rec >= not_rec and comment_class == \"recommended\") or (rec < not_rec and comment_class == \"not_recommended\"):\n",
    "        if(rec >= not_rec):\n",
    "            correct[0] += 1\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def test_with_pre_with_smooth(row, p, dictionary, all_of, correct, wrong):\n",
    "    comment_class = row['recommend']\n",
    "    lst = normalize_comment(row['comment'])\n",
    "    rec = p[RECOMMENDED]\n",
    "    not_rec = p[NOT_RECOMMENDED]\n",
    "    eval_dictionary_with_smoothing(dictionary, lst)\n",
    "    rec, not_rec = calc_p_with_smoothing(lst, p, dictionary)\n",
    "    if(rec >= not_rec):\n",
    "        all_of[0] += 1\n",
    "    if (rec >= not_rec and comment_class == \"recommended\") or (rec < not_rec and comment_class == \"not_recommended\"):\n",
    "        if(rec >= not_rec):\n",
    "            correct[0] += 1\n",
    "        return True\n",
    "    else:\n",
    "        wrong.append((row['title'], row['comment'], row['recommend']))\n",
    "        return False\n",
    "\n",
    "def test_without_pre_with_smooth(row, p, dictionary, all_of, correct):\n",
    "    sentence = row['comment']\n",
    "    comment_class = row['recommend']\n",
    "    lst =  word_tokenize(sentence)\n",
    "    rec = p[RECOMMENDED]\n",
    "    not_rec = p[NOT_RECOMMENDED]\n",
    "    eval_dictionary_with_smoothing(dictionary, lst)\n",
    "    rec, not_rec = calc_p_with_smoothing(lst, p, dictionary)\n",
    "    if(rec >= not_rec):\n",
    "        all_of[0] += 1\n",
    "    if (rec >= not_rec and comment_class == \"recommended\") or (rec < not_rec and comment_class == \"not_recommended\"):\n",
    "        if(rec >= not_rec):\n",
    "            correct[0] += 1\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def test_without_pre_without_smooth(row, p, rec_p, not_rec_p, all_of, correct):\n",
    "    sentence = row['comment']\n",
    "    comment_class = row['recommend']\n",
    "    lst =  word_tokenize(sentence)\n",
    "    rec, not_rec = calc_p_without_smoothing(p, lst, rec_p, not_rec_p)\n",
    "    if(rec >= not_rec):\n",
    "        all_of[0] += 1\n",
    "    if (rec >= not_rec and comment_class == \"recommended\") or (rec < not_rec and comment_class == \"not_recommended\"):\n",
    "        if(rec >= not_rec):\n",
    "            correct[0] += 1\n",
    "        return True\n",
    "    else:\n",
    "        return False   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>as you can see above, there is a function for every kind of prediction. we use these functions with apply function in pandas and apply it on every row in a Dataframeand in these functions if we predicted that it was recommended we increas variable 'all_of' and if it was correct we increase variable 'correct'</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3\n",
    "# 5)\n",
    "<strong>precision: to calculate the accuracy of our prediction, we divide all the right recommended types we predicted by all the recommended types (including the wrong ones). suppose we predicted only one of the comments and we predicted it correct then we get 100% accuracy but if we focus we'll know that we're wrong.<br><br>\n",
    "recall: we divide all the right recommended types we predicted by all the recommended types in the correct answer(which is given in the test file).if we predict all the types to be recommended then we will get 100% recall which is also not correct.<br>\n",
    "</strong>\n",
    "# 6)\n",
    "<strong>\n",
    "F1: we use harmonic mean for F1.The harmonic mean helps to find multiplicative or divisor relationships between fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(df, df_test, correct, all_of):\n",
    "    accuracy = formatted_float = \"{:.2f}\".format(100 * df.value_counts()[True] / df.count())\n",
    "    precision = formatted_float = \"{:.2f}\".format(100 * correct[0] / all_of[0])\n",
    "    recall = formatted_float = \"{:.2f}\".format(100 * correct[0] / df_test['recommend'].value_counts()['recommended'])\n",
    "    F1 = formatted_float = \"{:.2f}\".format(2 * (float(precision) * float(recall)) / (float(precision) + float(recall)))\n",
    "    print(\" accuracy : %\", accuracy)\n",
    "    print(\"precision : %\", precision)\n",
    "    print(\"   recall : %\", recall)\n",
    "    print(\"       F1 : %\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operation(op, wrong = None):\n",
    "    t1 = time.time()\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    df_train = pd.read_csv(\"comment_train.csv\")\n",
    "    rec_count = [0]\n",
    "    not_rec_count = [0]\n",
    "    all_of = [0]\n",
    "    correct = [0]\n",
    "    dictionary = [dict() for i in range(2)]\n",
    "    \n",
    "    if op == 'pre_smooth':\n",
    "        df_train.apply(lambda x : train_with_pre(x, dictionary, rec_count, not_rec_count), axis = 1)\n",
    "        count = df_train.count()[0]\n",
    "        p = [rec_count[0], not_rec_count[0]] / count\n",
    "        df_test = pd.read_csv(\"comment_test.csv\")\n",
    "        df = df_test.apply(lambda x : test_with_pre_with_smooth(x, p, dictionary, all_of, correct, wrong), axis = 1)\n",
    "        print(op, \":\")\n",
    "        print_results(df, df_test, correct, all_of)\n",
    "        \n",
    "    elif op == 'smooth':\n",
    "        df_train.apply(lambda x : train_without_pre(x, dictionary, rec_count, not_rec_count), axis = 1)\n",
    "        count = df_train.count()[0]\n",
    "        p = [rec_count[0], not_rec_count[0]] / count\n",
    "        df_test = pd.read_csv(\"comment_test.csv\")\n",
    "        df = df_test.apply(lambda x : test_without_pre_with_smooth(x, p, dictionary, all_of, correct), axis = 1)\n",
    "        print(op, \":\")\n",
    "        print_results(df, df_test, correct, all_of)\n",
    "        \n",
    "    elif op == 'pre':\n",
    "        df_train.apply(lambda x : train_with_pre(x, dictionary, rec_count, not_rec_count), axis = 1)\n",
    "        count = df_train.count()[0]\n",
    "        p = [rec_count[0], not_rec_count[0]] / count\n",
    "        rec_p = dict()\n",
    "        not_rec_p = dict()\n",
    "        rec_count = [0]\n",
    "        not_rec_count = [0]\n",
    "        for i in dictionary[RECOMMENDED]:\n",
    "            rec_count[0] += dictionary[RECOMMENDED][i]\n",
    "        for i in dictionary[NOT_RECOMMENDED]:\n",
    "            not_rec_count[0] += dictionary[NOT_RECOMMENDED][i]\n",
    "        for i in dictionary[RECOMMENDED]:\n",
    "            rec_p[i] = dictionary[RECOMMENDED][i] / rec_count[0]\n",
    "        for i in dictionary[NOT_RECOMMENDED]:\n",
    "            not_rec_p[i] = dictionary[NOT_RECOMMENDED][i] / not_rec_count[0]\n",
    "        df_test = pd.read_csv(\"comment_test.csv\")\n",
    "        df = df_test.apply(lambda x : test_with_pre_without_smooth(x, p, rec_p, not_rec_p, all_of, correct), axis = 1)\n",
    "        print(op, \":\")\n",
    "        print_results(df, df_test, correct, all_of)\n",
    "        \n",
    "    else:\n",
    "        df_train.apply(lambda x : train_without_pre(x, dictionary, rec_count, not_rec_count), axis = 1)\n",
    "        count = df_train.count()[0]\n",
    "        p = [rec_count[0], not_rec_count[0]] / count\n",
    "        rec_p = dict()\n",
    "        not_rec_p = dict()\n",
    "        rec_count = [0]\n",
    "        not_rec_count = [0]\n",
    "        for i in dictionary[RECOMMENDED]:\n",
    "            rec_count[0] += dictionary[RECOMMENDED][i]\n",
    "        for i in dictionary[NOT_RECOMMENDED]:\n",
    "            not_rec_count[0] += dictionary[NOT_RECOMMENDED][i]\n",
    "        for i in dictionary[RECOMMENDED]:\n",
    "            rec_p[i] = dictionary[RECOMMENDED][i] / rec_count[0]\n",
    "        for i in dictionary[NOT_RECOMMENDED]:\n",
    "            not_rec_p[i] = dictionary[NOT_RECOMMENDED][i] / not_rec_count[0]\n",
    "        df_test = pd.read_csv(\"comment_test.csv\")\n",
    "        df = df_test.apply(lambda x : test_without_pre_without_smooth(x, p, rec_p, not_rec_p, all_of, correct), axis = 1)\n",
    "        print(op, \":\")\n",
    "        print_results(df, df_test, correct, all_of)\n",
    "    print(\"     time : \", time.time() - t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7)\n",
    "<strong> from the answers shown below we find out that by the normalization that we do in our preprocess and additive smoothing, smoothing alone is almost better than doing both preprocess and additive smoothing and they both are far better than only doing preprocess or neither.\n",
    "<br>NOTE* : this conclusion is relative to the implementation of preprocess and additive smoothing and by doing some more preprocess like lemmatization, doing both of the operations may be better than doing only one of them or neither.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "pre_smooth :\n",
      " accuracy : % 90.50\n",
      "precision : % 90.30\n",
      "   recall : % 90.75\n",
      "       F1 : % 90.52\n",
      "     time :  6.5173656940460205 seconds\n"
     ]
    }
   ],
   "source": [
    "wrong = []\n",
    "operation('pre_smooth', wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "smooth :\n",
      " accuracy : % 91.25\n",
      "precision : % 91.25\n",
      "   recall : % 91.25\n",
      "       F1 : % 91.25\n",
      "     time :  6.554788112640381 seconds\n"
     ]
    }
   ],
   "source": [
    "operation('smooth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "pre :\n",
      " accuracy : % 86.12\n",
      "precision : % 80.29\n",
      "   recall : % 95.75\n",
      "       F1 : % 87.34\n",
      "     time :  2.504483222961426 seconds\n"
     ]
    }
   ],
   "source": [
    "operation('pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "none :\n",
      " accuracy : % 85.50\n",
      "precision : % 79.34\n",
      "   recall : % 96.00\n",
      "       F1 : % 86.88\n",
      "     time :  1.4043176174163818 seconds\n"
     ]
    }
   ],
   "source": [
    "operation('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>وری گود</td>\n",
       "      <td>تازه خریدم یه مدت کار بکنه مشخص میشه کیفیت قطعاتش</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>زیاد مناسب نیست رنگ پس میده یه وقتایی موقع نوشتن</td>\n",
       "      <td>با این قیمت گزینه های بهتری هم میشه گرفت.\\r\\nر...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>نقد پس از خرید</td>\n",
       "      <td>سلام ، راحت شدم از کابل شارژ ، توصیه میشود به ...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>خیالم راحت شد</td>\n",
       "      <td>فندک قبلیم مدام فیوز میسوزوند و یک بار شارژر م...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>خرید 2 عدد و مغایرت سریaو bو تفاوت دو مدل</td>\n",
       "      <td>، دوستان عزیز دقت کنند در مورد خرید این پاوربا...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title  \\\n",
       "0                                           وری گود   \n",
       "1  زیاد مناسب نیست رنگ پس میده یه وقتایی موقع نوشتن   \n",
       "2                                    نقد پس از خرید   \n",
       "3                                     خیالم راحت شد   \n",
       "4         خرید 2 عدد و مغایرت سریaو bو تفاوت دو مدل   \n",
       "\n",
       "                                             comment        recommend  \n",
       "0  تازه خریدم یه مدت کار بکنه مشخص میشه کیفیت قطعاتش      recommended  \n",
       "1  با این قیمت گزینه های بهتری هم میشه گرفت.\\r\\nر...  not_recommended  \n",
       "2  سلام ، راحت شدم از کابل شارژ ، توصیه میشود به ...      recommended  \n",
       "3  فندک قبلیم مدام فیوز میسوزوند و یک بار شارژر م...      recommended  \n",
       "4  ، دوستان عزیز دقت کنند در مورد خرید این پاوربا...  not_recommended  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_df = pd.DataFrame(wrong, columns =['title', 'comment', 'recommend']) \n",
    "wrong_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'با این قیمت گزینه های بهتری هم میشه گرفت.\\r\\nروان مینویسه ولی زیاد مناسب نیست و رنگ پس میده یه وقتایی موقع نوشتن'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see from above this comment was supposed to be not_recommended but we predicted that it would be recommended. the reason is that in the sentence : <br>\n",
    "با این قیمت گزینه های بهتری هم میشه گرفت.\\r\\nروان مینویسه ولی زیاد مناسب نیست و رنگ پس میده یه وقتایی موقع نوشتن\n",
    "<br>\n",
    "it has a negative meaning but the words in the comment mostly are not negative like: مناسب will be taken as a positive adj or بهتری will be changed to بهتر which is positive, these things will cause a comment to be predicted wrongly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
